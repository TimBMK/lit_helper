{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination tests\n",
    "\n",
    "Some tests how well an agent does when given multiple tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Zotero settings\n",
    "load_dotenv()\n",
    "zotero_api_key = os.environ.get(\"ZOTERO_API_KEY_READ\") # A Zotero API key, here sourced from .env - read only access is enough\n",
    "zotero_library_id = os.environ.get(\"ZOTERO_TEST_GROUP_ID\") # The library ID of the Zotero library to be accessed. Here a test group library\n",
    "zotero_library_type = \"group\" # set the library type to either 'user' or 'group'. Make sure this corresponds to the library ID ('user' for your personal library)\n",
    "zotero_docs_returned = 20 # number of documents returned from the zotero library\n",
    "zot_get_fulltext = False # return full text from documents in the zotero library, if available\n",
    "zot_type = \"top\" # \"top\" (top level items) or \"items\" (incl. sub level items, e.g. deleted)\n",
    "\n",
    "# PDF Retriever Settings\n",
    "lit_directory = \"./test_data/\" # directory with literature to load for the RAG\n",
    "db_directory = \"./test_chroma_db\" # directory to save the vector store\n",
    "pattern = r\"[^\\\\/]*[\\\\/](?P<author>.+?) (?P<year>\\d{4}) (?P<title>.+)\\.pdf\" # regex pattern to extract metadata from naming scheme in literature directory. Adjust as needed!\n",
    "short_docs = True # shorten document from pages to chunks? (set chunk size below)\n",
    "docs_returned = 6 # number of docs returned by the retriever(s)\n",
    "retriever_type = \"mmr\" # \"similarity\"  or \"mmr\"\n",
    "\n",
    "\n",
    "# Model settings\n",
    "\n",
    "embedding_model = \"nomic-embed-text\" # very good embedding model for retrieval tasks\n",
    "\n",
    "# textgen model. generates answers to questions, with retrieved documents as context\n",
    "# textgen_model = \"llama3.2\"\n",
    "textgen_model = \"o3-mini-2025-01-31\"\n",
    "# textgen_model = \"phi4\", # phi4 is a rather powerful model, but requires more gpu compute (that is, it will be slower than llama3.2 if not enough gpu memory is available)\n",
    "\n",
    "# reasoning model. handles deciding whether or not to query the retriever\n",
    "# reasoning_model = \"nemotron-mini\" # nvidia's nemotron-mini is smaller, but still capable of the simple reasoning task we need (retrieve or not)\n",
    "# reasoning_model = \"llama3.2\" # fast, but not very good at reasoning tasks\n",
    "reasoning_model = \"o3-mini-2025-01-31\"\n",
    "# reasoning_model = \"qwq\" # qwq is a rather powerful experimental reasoning model. However, as it is rather large (20b), it either requires a fair bit of GPU memory or will be very slow \n",
    "\n",
    "# selfquery model. generates queries for the retriever\n",
    "# selfquery_model = \"phi4\" # phi4 seems better at query construction than llama3.2, but requires more gpu compute (i.e. may be slower)\n",
    "# selfquery_model = \"deepseek-r1:1.5b\"\n",
    "selfquery_model = \"o3-mini-2025-01-31\"\n",
    "\n",
    "\n",
    "max_tokens = 2048 # maximum number of tokens to generate across models\n",
    "temperature = 0 # temperature across models\n",
    "\n",
    "verbose = True # print additional information on routing decisions etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature parameter not supported for model o3-mini-2025-01-31.\n",
      "Temperature parameter not supported for model o3-mini-2025-01-31.\n",
      "Temperature parameter not supported for model o3-mini-2025-01-31.\n"
     ]
    }
   ],
   "source": [
    "# set up the models\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\")) # get API key from .env file\n",
    "\n",
    "openai_models = [model.id for model in client.models.list()]\n",
    "\n",
    "models = [textgen_model, reasoning_model, selfquery_model]\n",
    "names = [\"textgen_llm\", \"reasoning_llm\", \"selfquery_llm\"]\n",
    "for name, model in zip(names, models):\n",
    "    if model in openai_models:\n",
    "        if \"o3\" in model or o1 in model:\n",
    "            print(f\"Temperature parameter not supported for model {model}.\")\n",
    "            llm = ChatOpenAI(\n",
    "                        model = model,\n",
    "                        max_tokens = max_tokens, # max number of tokens to generate\n",
    "                    )\n",
    "        else:\n",
    "            llm = ChatOpenAI(\n",
    "                model = model,\n",
    "                temperature=temperature,\n",
    "                max_tokens = max_tokens, # max number of tokens to generate\n",
    "            )\n",
    "      \n",
    "    else:\n",
    "        llm = ChatOllama(\n",
    "            model = model,\n",
    "            temperature = temperature,\n",
    "            num_predict = max_tokens, # max number of tokens to generate\n",
    "        )\n",
    "    globals()[f\"{name}\"] = llm\n",
    "\n",
    "embeddings = OllamaEmbeddings(model = embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the zotero retriever\n",
    "\n",
    "from pyzotero import zotero\n",
    "## get local ZoteroRetriever implementation until it is added to langchain_community\n",
    "import sys\n",
    "sys.path.append(\"D:/academicCloud/Python Scripts/langchain-zotero-retriever/src\")\n",
    "from langchain_zotero_retriever.retrievers import ZoteroRetriever\n",
    "\n",
    "zot = zotero.Zotero(zotero_library_id, zotero_library_type, zotero_api_key) # zot for direct lib access\n",
    "\n",
    "## pass settings for retriever\n",
    "zotero_retriever = ZoteroRetriever(k = zotero_docs_returned, \n",
    "                                   get_fulltext = zot_get_fulltext, \n",
    "                                   library_id = zotero_library_id,\n",
    "                                   library_type = zotero_library_type,\n",
    "                                   api_key = zotero_api_key,\n",
    "                                   type = zot_type,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the PDF vector store\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from re import match\n",
    "import os\n",
    "\n",
    "## check if the folder db_directory already exists. If not, creat it and load the documents into the vector store. Else, use the existing vector store. Makes a new DB for shortened docs\n",
    "\n",
    "if short_docs:\n",
    "    db_directory += \"_short\"\n",
    "\n",
    "if not os.path.exists(db_directory):\n",
    "    # load documents\n",
    "    loader = PyPDFDirectoryLoader(lit_directory)\n",
    "    docs = loader.load() # metadata tracks paper and page number; each page is a single document\n",
    "\n",
    "    # extract metadata from file names (according to the pattern specified above)\n",
    "    docs = [\n",
    "        Document(\n",
    "            doc.page_content,\n",
    "            metadata={\n",
    "                **doc.metadata,\n",
    "                **({\n",
    "                    \"author_name\": metadata.group(\"author\"),\n",
    "                    \"year\": metadata.group(\"year\"),\n",
    "                    \"title\": metadata.group(\"title\")\n",
    "                } if (metadata := match(pattern, doc.metadata[\"source\"])) else {})\n",
    "            }\n",
    "        )\n",
    "        for doc in docs\n",
    "    ]\n",
    "\n",
    "\n",
    "    # optional step: split the docs into smaller chunks to fit into context window of the model (model dependant, necessary for small models) -!! test this, shorter chunks may lead to bad retrieval results !!-\n",
    "    #           potential remedy: use whole pages, but use the model to summarise each page before chaining it into the context\n",
    "    if short_docs:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,  # chunk size (characters)\n",
    "            chunk_overlap=200,  # chunk overlap (characters)\n",
    "            add_start_index=True,  # track index in original document\n",
    "            )\n",
    "        \n",
    "        docs = text_splitter.split_documents(docs)\n",
    "\n",
    "    # make doc vector store. as the vector store can get quite large (and takes time to initialize in memory), we use a chroma database to store the vectors    \n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"lit_helper_test\",\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=db_directory,  # save data locally\n",
    "        )\n",
    "    \n",
    "    vector_store.add_documents(docs) # add docs\n",
    "\n",
    "else:\n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"lit_helper_test\",\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=db_directory,  # save data locally\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your goal is to structure the user's query to match the request schema provided below.\n",
      "\n",
      "<< Structured Request Schema >>\n",
      "When responding use a markdown code snippet with a JSON object formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"query\": string \\ text string to compare to document contents\n",
      "    \"filter\": string \\ logical condition statement for filtering documents\n",
      "}\n",
      "```\n",
      "\n",
      "The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.\n",
      "\n",
      "A logical condition statement is composed of one or more comparison and logical operation statements.\n",
      "\n",
      "A comparison statement takes the form: `comp(attr, val)`:\n",
      "- `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator\n",
      "- `attr` (string):  name of attribute to apply the comparison to\n",
      "- `val` (string): is the comparison value\n",
      "\n",
      "A logical operation statement takes the form `op(statement1, statement2, ...)`:\n",
      "- `op` (and | or | not): logical operator\n",
      "- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\n",
      "\n",
      "Make sure that you only use the comparators and logical operators listed above and no others.\n",
      "Make sure that filters only refer to attributes that exist in the data source.\n",
      "Make sure that filters only use the attributed names with its function names if there are functions applied on them.\n",
      "Make sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\n",
      "Make sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\n",
      "Make sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.\n",
      "\n",
      "<< Example 1. >>\n",
      "Data Source:\n",
      "```json\n",
      "{\n",
      "    \"content\": \"Lyrics of a song\",\n",
      "    \"attributes\": {\n",
      "        \"artist\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"Name of the song artist\"\n",
      "        },\n",
      "        \"length\": {\n",
      "            \"type\": \"integer\",\n",
      "            \"description\": \"Length of the song in seconds\"\n",
      "        },\n",
      "        \"genre\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "User Query:\n",
      "What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genre\n",
      "\n",
      "Structured Request:\n",
      "```json\n",
      "{\n",
      "    \"query\": \"teenager love\",\n",
      "    \"filter\": \"and(or(eq(\\\"artist\\\", \\\"Taylor Swift\\\"), eq(\\\"artist\\\", \\\"Katy Perry\\\")), lt(\\\"length\\\", 180), eq(\\\"genre\\\", \\\"pop\\\"))\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "<< Example 2. >>\n",
      "Data Source:\n",
      "```json\n",
      "{\n",
      "    \"content\": \"Lyrics of a song\",\n",
      "    \"attributes\": {\n",
      "        \"artist\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"Name of the song artist\"\n",
      "        },\n",
      "        \"length\": {\n",
      "            \"type\": \"integer\",\n",
      "            \"description\": \"Length of the song in seconds\"\n",
      "        },\n",
      "        \"genre\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "User Query:\n",
      "What are songs that were not published on Spotify\n",
      "\n",
      "Structured Request:\n",
      "```json\n",
      "{\n",
      "    \"query\": \"\",\n",
      "    \"filter\": \"NO_FILTER\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "<< Example 3. >>\n",
      "Data Source:\n",
      "```json\n",
      "{\n",
      "    \"content\": \"A passage from a scientific paper\",\n",
      "    \"attributes\": {\n",
      "    \"author_name\": {\n",
      "        \"description\": \"The paper author's last name. Abbreviated with et al. for more than two authors.\",\n",
      "        \"type\": \"string\"\n",
      "    },\n",
      "    \"year\": {\n",
      "        \"description\": \"The year of publication\",\n",
      "        \"type\": \"string\"\n",
      "    },\n",
      "    \"title\": {\n",
      "        \"description\": \"The title of the paper\",\n",
      "        \"type\": \"string\"\n",
      "    },\n",
      "    \"page\": {\n",
      "        \"description\": \"The page number of the paper\",\n",
      "        \"type\": \"string\"\n",
      "    },\n",
      "    \"source\": {\n",
      "        \"description\": \"The file path of the paper\",\n",
      "        \"type\": \"string\"\n",
      "    }\n",
      "}\n",
      "}\n",
      "```\n",
      "\n",
      "User Query:\n",
      "{query}\n",
      "\n",
      "Structured Request:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set up the self querying retriever\n",
    "\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.chains.query_constructor.base import get_query_constructor_prompt\n",
    "from langchain.chains.query_constructor.base import StructuredQueryOutputParser\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_community.query_constructors.chroma import ChromaTranslator\n",
    "\n",
    "metadata_field_info = [\n",
    "    # note that the attribute infos can have a vast influence on the performance of the retriever (likely model dependant)\n",
    "    #   The name seems to be most important when the model chooses the filter - with notable side effects for other attributes when changing the name of one!\n",
    "    #   Beware that the names here need to correspond to the names in the metadata of the documents set above (changes in the name here require updating the chroma database)\n",
    "    AttributeInfo(\n",
    "        name = \"author_name\",\n",
    "        description=\"The paper author's last name. Abbreviated with et al. for more than two authors.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name = \"year\",\n",
    "        description=\"The year of publication\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name = \"title\",\n",
    "        description=\"The title of the paper\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name = \"page\",\n",
    "        description=\"The page number of the paper\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name = \"source\",\n",
    "        description=\"The file path of the paper\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"A passage from a scientific paper\"\n",
    "\n",
    "\n",
    "retrieval_prompt = get_query_constructor_prompt( # we're using a premade prompt for the query constructor\n",
    "    document_contents=document_content_description,\n",
    "    attribute_info=metadata_field_info,\n",
    "    # examples = retriever examples, # we can optionally add examples for the model to fine tune the retrieval. See https://github.com/langchain-ai/langchain/blob/master/cookbook/self_query_hotel_search.ipynb\n",
    ")\n",
    "\n",
    "output_parser = StructuredQueryOutputParser.from_components( # we're also using a premade output parser\n",
    "    fix_invalid=True, # this automatically fixes invalid queries (that is, it should avoid errors being thrown when the model constructs an invalid query)\n",
    "    ) \n",
    "\n",
    "query_constructor = retrieval_prompt | selfquery_llm | output_parser # make the query constructor chain\n",
    "\n",
    "## pass settings (adjust as needed)\n",
    "if retriever_type == \"mmr\":\n",
    "    search_type=\"mmr\" # MMR (Maximal Marginal Relevance) aims to diversify search results. the amount of diversification is set via the lambda_mult parameter\n",
    "    search_kwargs={\"k\": docs_returned, # make sure the number of documents passed (k) fits into the context window\n",
    "                       \"fetch_k\": docs_returned * 5, # could be adjusted, potentially run tests\n",
    "                       \"lambda_mult\": 0.8 # amount of diversification, with 0 being maximum diversity\n",
    "                       }\n",
    "\n",
    "if retriever_type == \"similarity\":\n",
    "    search_type=\"similarity\" # similarity score; optionally with threshold (\"similarity_score_threshold\" with \"score_threshold\" kwarg)\n",
    "    search_kwargs={\"k\": docs_returned} # make sure the number of documents passed (k) fits into the context window\n",
    "\n",
    "\n",
    "self_query_retriever = SelfQueryRetriever(\n",
    "    query_constructor=query_constructor,\n",
    "    vectorstore=vector_store,\n",
    "    structured_query_translator=ChromaTranslator(), # we need to specify the translator for our database scheme, e.g. Chroma\n",
    "    search_type=search_type,\n",
    "    search_kwargs=search_kwargs, \n",
    ")\n",
    "\n",
    "if verbose: print(retrieval_prompt.format(query=\"{query}\")) # print out the prompt used for the query constructor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lit_helper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
